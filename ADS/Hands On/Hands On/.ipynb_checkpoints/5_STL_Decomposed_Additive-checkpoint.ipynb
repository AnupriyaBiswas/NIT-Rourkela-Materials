{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38be523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ae4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten,LSTM\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.metrics import RootMeanSquaredError,mean_absolute_percentage_error\n",
    "from keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeccf6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate time series into patterns\n",
    "def get_Patterns(TSeries, n_inputs,h):\n",
    "    X,y,z = pd.DataFrame(np.zeros((len(TSeries)-n_inputs-h+1,n_inputs))), pd.DataFrame(), pd.DataFrame()\n",
    "    for i in range(len(TSeries)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_inputs + h - 1\n",
    "        # check if we are beyond the time series\n",
    "        if end_ix > len(TSeries)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        for j in range(n_inputs):\n",
    "            X.loc[i,j]=TSeries.iloc[i+j,0]\n",
    "        i=i+n_inputs\n",
    "        #y=y.append(TSeries.iloc[end_ix], ignore_index = True)\n",
    "        y=pd.concat([y, TSeries.iloc[end_ix]], ignore_index=True)\n",
    "    return pd.DataFrame(X),pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa9c68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# originalData should be a Column Vectored DataFrame\n",
    "def minmaxNorm(originalData, lenTrainValidation):\n",
    "    # Maximum Value\n",
    "    max2norm=max(originalData.iloc[0:lenTrainValidation,0])\n",
    "    # Minimum Value\n",
    "    min2norm=min(originalData.iloc[0:lenTrainValidation,0])\n",
    "    lenOriginal=len(originalData)\n",
    "    normalizedData=np.zeros(lenOriginal)   \n",
    "    normalizedData = []\n",
    "    #Normalize using Min-Max Normalization\n",
    "    for i in range (lenOriginal):\n",
    "        normalizedData.append((originalData.iloc[i]-min2norm)/(max2norm-min2norm+0.00001))    \n",
    "    return pd.DataFrame(normalizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c481549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# originalData and forecastedData should be Column Vectored DataFrames\n",
    "def minmaxDeNorm( originalData, forecastedData, lenTrainValidation):\n",
    "    # Maximum Value\n",
    "    max2norm=max(originalData.iloc[0:lenTrainValidation,0])\n",
    "    # Minimum Value\n",
    "    min2norm=min(originalData.iloc[0:lenTrainValidation,0])\n",
    "    lenOriginal=len(originalData)\n",
    "    denormalizedData=[]   \n",
    "    #De-Normalize using Min-Max Normalization\n",
    "    for i in range (lenOriginal):\n",
    "        denormalizedData.append((forecastedData.iloc[i]*(max2norm-min2norm))+min2norm)  \n",
    "    return pd.DataFrame(denormalizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5142a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries_Data and forecasted_value should be Column Vectored DataFrames\n",
    "def findRMSE( Timeseries_Data, forecasted_value,lenTrainValidation):\n",
    "    l=Timeseries_Data.shape[0]\n",
    "    lenTest=l-lenTrainValidation\n",
    "    trainRMSE=0;\n",
    "    for i in range (lenTrainValidation):\n",
    "        trainRMSE=trainRMSE+np.power((forecasted_value.iloc[i,0]-Timeseries_Data.iloc[i,0]),2) \n",
    "    trainRMSE=np.sqrt(trainRMSE/lenTrainValidation)\n",
    "\n",
    "    testRMSE=0;\n",
    "    for i in range (lenTrainValidation,l,1):\n",
    "        testRMSE=testRMSE+np.power((forecasted_value.iloc[i,0]-Timeseries_Data.iloc[i,0]),2)\n",
    "    testRMSE=np.sqrt(testRMSE/lenTest)\n",
    "    return trainRMSE, testRMSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9147158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries_Data and forecasted_value should be Column Vectored DataFrames\n",
    "def findMAE( Timeseries_Data, forecasted_value,lenTrainValidation):\n",
    "    l=Timeseries_Data.shape[0]\n",
    "    lenTest=l-lenTrainValidation\n",
    "    trainMAE=0;\n",
    "    for i in range (lenTrainValidation):\n",
    "        trainMAE=trainMAE+np.abs(forecasted_value.iloc[i,0]-Timeseries_Data.iloc[i,0]) \n",
    "    trainMAE=(trainMAE/(lenTrainValidation));\n",
    "\n",
    "    testMAE=0;\n",
    "    for i in range (lenTrainValidation,l,1):\n",
    "        testMAE=testMAE+np.abs(forecasted_value.iloc[i,0]-Timeseries_Data.iloc[i,0])\n",
    "    testMAE=(testMAE/lenTest);\n",
    "    return trainMAE, testMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fde0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Fitness_LSTM(x,y,lenValid,lenTest):\n",
    "    NOP=len(y)\n",
    "    n_features=1\n",
    "    lenTrain=NOP-lenValid-lenTest\n",
    "    xTrain=x.iloc[0:lenTrain,:]\n",
    "    xValid=x.iloc[lenTrain:(lenTrain+lenValid),:]\n",
    "    xTest=x.iloc[(lenTrain+lenValid):NOP,:]\n",
    "    yTrain=y.iloc[0:lenTrain,0]\n",
    "    yValid=y.iloc[lenTrain:(lenTrain+lenValid),0]\n",
    "    yTest=y.iloc[(lenTrain+lenValid):NOP,0]\n",
    "    model1 = Sequential()\n",
    "    model1.add(LSTM(256, activation='relu', input_shape=(LagLength,n_features)))\n",
    "    model1.add(Flatten())\n",
    "    model1.add(Dense(128, activation='relu'))\n",
    "    model1.add(Dense(64, activation='relu'))\n",
    "    model1.add(Dense(32, activation='relu'))\n",
    "    model1.add(Dense(1,activation='linear'))\n",
    "    checkpoint_filepath = '/cp.keras'\n",
    "    cp1=ModelCheckpoint(checkpoint_filepath,save_best_only='True')\n",
    "    model1.compile(loss=MeanSquaredError(),optimizer=Adam(learning_rate=0.01),metrics=[RootMeanSquaredError(),mean_absolute_percentage_error])\n",
    "    # fit\n",
    "    model1.fit(xTrain, yTrain,validation_data=(xValid,yValid), epochs=50, verbose=1)\n",
    "    yhatNorm=model1.predict(x).flatten().reshape(x.shape[0],1)\n",
    "    return pd.DataFrame(yhatNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae944f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Fitness(x,y,lenValid,lenTest,model):\n",
    "    NOP=y.shape[0]\n",
    "    lenTrain=NOP-lenValid-lenTest\n",
    "    xTrain=x.iloc[0:lenTrain,:]\n",
    "    xValid=x.iloc[lenTrain:(lenTrain+lenValid),:]\n",
    "    xTest=x.iloc[(lenTrain+lenValid):NOP,:]\n",
    "    yTrain=y.iloc[0:lenTrain,0]\n",
    "    yValid=y.iloc[lenTrain:(lenTrain+lenValid),0]\n",
    "    yTest=y.iloc[(lenTrain+lenValid):NOP,0]\n",
    "    model.fit(xTrain, yTrain)\n",
    "    yhatNorm=model.predict(x).flatten().reshape(x.shape[0],1)\n",
    "    return pd.DataFrame(yhatNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e6a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "            0           1           2       3\n",
      "1  362.200445  349.009016  354.823529  348.44\n"
     ]
    }
   ],
   "source": [
    "#Read the Time Series Dataset\n",
    "Timeseries_Data=pd.read_csv('AQI.csv',header=None)\n",
    "LagLength=24\n",
    "h=1\n",
    "lt=Timeseries_Data.shape[0]\n",
    "lenTrain=int(round(lt*0.7))\n",
    "lenValidation=int(round(lt*0.15))\n",
    "lenTest=int(lt-lenTrain-lenValidation)\n",
    "# For additive STL Decomposition\n",
    "# result = seasonal_decompose(Timeseries_Data, model='additive', period = 1)\n",
    "# For multiplicative STL Decomposition\n",
    "result = seasonal_decompose(Timeseries_Data, model='multiplicative', period = 1)\n",
    "\n",
    "T=pd.DataFrame(result.trend)\n",
    "S=pd.DataFrame(result.seasonal)\n",
    "R=pd.DataFrame(result.resid)\n",
    "T=T.fillna(0)\n",
    "S=S.fillna(0)\n",
    "R=R.fillna(0)\n",
    "R.rename(columns = {'resid':0}, inplace = True)\n",
    "T.rename(columns = {'trend':0}, inplace = True)\n",
    "S.rename(columns = {'seasonal':0}, inplace = True)\n",
    "\n",
    "\n",
    "# Forecasts on R\n",
    "# NORMALIZE THE DATA\n",
    "normalizedData=minmaxNorm(R,lenTrain+lenValidation);\n",
    "# Transform the Time Series into Patterns Using Sliding Window\n",
    "X, y = get_Patterns(normalizedData, LagLength, h)\n",
    "model=LinearRegression()\n",
    "name='LinearRegression'\n",
    "file1='./'+str(name)+\"_Accuracy.xlsx\"\n",
    "file2='./'+str(name)+\"_Forecasts.xlsx\"\n",
    "ynorm1=Find_Fitness(X,y,lenValidation,lenTest,model)\n",
    "ynorm=pd.DataFrame(normalizedData.iloc[0:(LagLength+h-1),0])\n",
    "ynorm=pd.concat([ynorm, ynorm1], ignore_index=True)\n",
    "yhat=minmaxDeNorm(R, ynorm, lenTrain+lenValidation)\n",
    "Rhat=yhat\n",
    "print('Ok')\n",
    "\n",
    "# Forecasts on T\n",
    "# NORMALIZE THE DATA\n",
    "normalizedData=minmaxNorm(T,lenTrain+lenValidation);\n",
    "# Transform the Time Series into Patterns Using Sliding Window\n",
    "X, y = get_Patterns(normalizedData, LagLength, h)\n",
    "model=LinearRegression()\n",
    "name='LinearRegression'\n",
    "file1='./'+str(name)+\"_Accuracy.xlsx\"\n",
    "file2='./'+str(name)+\"_Forecasts.xlsx\"\n",
    "ynorm1=Find_Fitness(X,y,lenValidation,lenTest,model)\n",
    "ynorm=pd.DataFrame(normalizedData.iloc[0:(LagLength+h-1),0])\n",
    "ynorm=pd.concat([ynorm, ynorm1], ignore_index=True)\n",
    "That=yhat\n",
    "\n",
    "# Forecasts on S\n",
    "# NORMALIZE THE DATA\n",
    "normalizedData=minmaxNorm(S,lenTrain+lenValidation);\n",
    "# Transform the Time Series into Patterns Using Sliding Window\n",
    "X, y = get_Patterns(normalizedData, LagLength, h)\n",
    "model=LinearRegression()\n",
    "name='LinearRegression'\n",
    "file1='./'+str(name)+\"_Accuracy.xlsx\"\n",
    "file2='./'+str(name)+\"_Forecasts.xlsx\"\n",
    "\n",
    "\n",
    "Accuracy=pd.DataFrame()\n",
    "\n",
    "ynorm1=Find_Fitness(X,y,lenValidation,lenTest,model)\n",
    "ynorm=pd.DataFrame(normalizedData.iloc[0:(LagLength+h-1),0])\n",
    "ynorm=pd.concat([ynorm, ynorm1], ignore_index=True)\n",
    "yhat=minmaxDeNorm(S, ynorm, lenTrain+lenValidation)\n",
    "Shat=yhat\n",
    "\n",
    "yhat=Rhat*Shat*That\n",
    "\n",
    "Accuracy.loc[1,0],Accuracy.loc[1,1]=findRMSE( Timeseries_Data,yhat,lenTrain+lenValidation)\n",
    "Accuracy.loc[1,2],Accuracy.loc[1,3]=findMAE( Timeseries_Data,yhat,lenTrain+lenValidation)\n",
    "Accuracy.to_excel(file1,sheet_name='Accuracy',index=False)\n",
    "yhat.to_excel(file2,sheet_name='Forecasts',index=False)\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad3425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
